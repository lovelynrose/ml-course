{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kl0D7YLmnza6",
        "outputId": "9ae81dfc-936e-470e-b856-6c45d2f2f039"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: Partial Derivative = -0.06643, Updated w = 0.00566\n",
            "Iteration 2: Partial Derivative = -0.06640, Updated w = 0.00633\n",
            "Iteration 3: Partial Derivative = -0.06637, Updated w = 0.00699\n",
            "Iteration 4: Partial Derivative = -0.06634, Updated w = 0.00766\n",
            "Iteration 5: Partial Derivative = -0.06631, Updated w = 0.00832\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.008318567890877268"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Define the function to perform gradient descent with specified iterations\n",
        "def gradient_descent(x, y, w_init, learning_rate, iterations):\n",
        "    w = w_init\n",
        "    for i in range(iterations):\n",
        "        # Calculate predictions\n",
        "        y_pred = [w * xi for xi in x]\n",
        "        # Calculate the gradient of the loss function with respect to predictions\n",
        "        dL_dy_pred = [(yp - yi) for yp, yi in zip(y_pred, y)]\n",
        "        # Calculate the gradient of the loss with respect to w\n",
        "        gradient = sum(dL_dyp * xi for dL_dyp, xi in zip(dL_dy_pred, x))\n",
        "        gradient = gradient/3\n",
        "        # Update the weight\n",
        "        w -= learning_rate * gradient\n",
        "        # Display the partial derivative (gradient) and the updated weight\n",
        "        print(f\"Iteration {i+1}: Partial Derivative = {gradient:.5f}, Updated w = {w:.5f}\")\n",
        "    return w\n",
        "\n",
        "# Given values\n",
        "x = [0.1, 0.2, 0.3]\n",
        "y = [0.2, 0.3, 0.4]\n",
        "w_init = 0.005\n",
        "learning_rate = 0.01\n",
        "iterations = 5  # Specify the number of iterations\n",
        "\n",
        "# Perform gradient descent\n",
        "gradient_descent(x, y, w_init, learning_rate, iterations)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Single unknown(w)"
      ],
      "metadata": {
        "id": "9R0H1i_PsRh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the function to perform gradient descent with specified iterations\n",
        "def gradient_descent(x, y, w_init, learning_rate, iterations):\n",
        "    w = w_init\n",
        "    for i in range(iterations):\n",
        "\n",
        "        # Calculate the gradient of the loss with respect to w\n",
        "        gradient = sum((w * xi - yi) * xi for yi, xi in zip(y, x))\n",
        "        gradient = gradient/3\n",
        "\n",
        "        # Update the weight\n",
        "        w -= gradient\n",
        "        # Display the partial derivative (gradient) and the updated weight\n",
        "        print(f\"Iteration {i+1}: Partial Derivative = {gradient:.5f}, Updated w = {w:.5f}\")\n",
        "    return w\n",
        "\n",
        "# Given values\n",
        "x = [0.1, 0.2, 0.3]\n",
        "y = [0.2, 0.3, 0.4]\n",
        "w_init = 0.05\n",
        "#learning_rate = 0.01\n",
        "iterations = 5  # Specify the number of iterations\n",
        "\n",
        "# Perform gradient descent\n",
        "gradient_descent(x, y, w_init, learning_rate, iterations)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWFUu3PXqGnQ",
        "outputId": "2f5b8a80-703b-4c98-d224-3d098a0b9ddc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: Partial Derivative = -0.06433, Updated w = 0.11433\n",
            "Iteration 2: Partial Derivative = -0.06133, Updated w = 0.17566\n",
            "Iteration 3: Partial Derivative = -0.05847, Updated w = 0.23413\n",
            "Iteration 4: Partial Derivative = -0.05574, Updated w = 0.28987\n",
            "Iteration 5: Partial Derivative = -0.05314, Updated w = 0.34301\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3430130957320165"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2 unknown(w0 and w1)"
      ],
      "metadata": {
        "id": "le7JyyRasYsa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the function to perform gradient descent with specified iterations\n",
        "def gradient_descent(x, y, w0_init, w1_init, learning_rate, iterations):\n",
        "    w0 = w0_init\n",
        "    w1 = w1_init\n",
        "    for i in range(iterations):\n",
        "\n",
        "        # Calculate the gradient of the loss with respect to w0\n",
        "        w0_gradient = sum((w0 + w1 * xi - yi) for yi, xi in zip(y, x))\n",
        "        w0_gradient = w0_gradient/3\n",
        "\n",
        "        # Calculate the gradient of the loss with respect to w1\n",
        "        w1_gradient = sum((w0 + w1 * xi - yi) * xi for yi, xi in zip(y, x))\n",
        "        w1_gradient = w1_gradient/3\n",
        "\n",
        "        # Update the weight\n",
        "        w0 -= w0_gradient\n",
        "        w1 -= w1_gradient\n",
        "        # Display the partial derivative (gradient) and the updated weight\n",
        "        print(f\"Iteration {i+1}: Partial Derivative = {w0_gradient:.5f}, Updated w0 = {w0:.5f}\")\n",
        "\n",
        "        # Display the partial derivative (gradient) and the updated weight\n",
        "        print(f\"Iteration {i+1}: Partial Derivative = {w1_gradient:.5f}, Updated w1 = {w1:.5f}\")\n",
        "    return w0, w1\n",
        "\n",
        "# Given values\n",
        "x = [0.1, 0.2, 0.3]\n",
        "y = [0.2, 0.3, 0.4]\n",
        "w0_init = 1\n",
        "w1_init = 0.05\n",
        "#learning_rate = 0.01\n",
        "iterations = 5  # Specify the number of iterations\n",
        "\n",
        "# Perform gradient descent\n",
        "gradient_descent(x, y, w0_init, w1_init, learning_rate, iterations)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycZIWqYcscAP",
        "outputId": "ee4e2e44-c680-4390-de9f-47715467d4b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: Partial Derivative = 0.71000, Updated w0 = 0.29000\n",
            "Iteration 1: Partial Derivative = 0.13567, Updated w1 = -0.08567\n",
            "Iteration 2: Partial Derivative = -0.02713, Updated w0 = 0.31713\n",
            "Iteration 2: Partial Derivative = -0.01266, Updated w1 = -0.07300\n",
            "Iteration 3: Partial Derivative = 0.00253, Updated w0 = 0.31460\n",
            "Iteration 3: Partial Derivative = -0.00665, Updated w1 = -0.06636\n",
            "Iteration 4: Partial Derivative = 0.00133, Updated w0 = 0.31327\n",
            "Iteration 4: Partial Derivative = -0.00684, Updated w1 = -0.05951\n",
            "Iteration 5: Partial Derivative = 0.00137, Updated w0 = 0.31190\n",
            "Iteration 5: Partial Derivative = -0.00679, Updated w1 = -0.05272\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.3119024572641975, -0.05272259770008227)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(x, y, w0_init, w1_init, learning_rate, iterations):\n",
        "    w0 = w0_init\n",
        "    w1 = w1_init\n",
        "    n = len(x)  # Number of data points\n",
        "    for i in range(iterations):\n",
        "        # Calculate predictions\n",
        "        y_pred = [w0 + w1 * xi for xi in x]\n",
        "        # Calculate the gradients for w0 and w1\n",
        "        w0_gradient = sum((yp - yi) for yp, yi in zip(y_pred, y)) / n\n",
        "        w1_gradient = sum(((yp - yi) * xi) for yp, yi, xi in zip(y_pred, y, x)) / n\n",
        "        # Update the weights using the gradients\n",
        "        w0 -= learning_rate * w0_gradient\n",
        "        w1 -= learning_rate * w1_gradient\n",
        "        # Calculate the Mean Squared Error (MSE) for the current iteration\n",
        "        mse = sum((yi - yp)**2 for yi, yp in zip(y, y_pred)) / n\n",
        "        # Display the MSE for the current iteration\n",
        "        print(f\"Iteration {i+1}: MSE = {mse:.5f}\")\n",
        "    return w0, w1\n",
        "\n",
        "# Given values for the problem\n",
        "x = [0.1, 0.2, 0.3]  # Input features\n",
        "y = [0.2, 0.3, 0.4]  # Target values\n",
        "w0_init = 1  # Initial weight for the bias (intercept)\n",
        "w1_init = 0.05  # Initial weight for the feature\n",
        "learning_rate = 1  # Learning rate for the gradient descent\n",
        "iterations = 15  # Number of iterations to perform\n",
        "\n",
        "# Call the function to perform gradient descent and find MSE after every iteration\n",
        "gradient_descent(x, y, w0_init, w1_init, learning_rate, iterations)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMgyx-OxA0iv",
        "outputId": "3fc28e1a-a249-4b52-d941-84108eb63c3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: MSE = 0.51012\n",
            "Iteration 2: MSE = 0.00859\n",
            "Iteration 3: MSE = 0.00768\n",
            "Iteration 4: MSE = 0.00758\n",
            "Iteration 5: MSE = 0.00749\n",
            "Iteration 6: MSE = 0.00739\n",
            "Iteration 7: MSE = 0.00730\n",
            "Iteration 8: MSE = 0.00720\n",
            "Iteration 9: MSE = 0.00711\n",
            "Iteration 10: MSE = 0.00702\n",
            "Iteration 11: MSE = 0.00693\n",
            "Iteration 12: MSE = 0.00684\n",
            "Iteration 13: MSE = 0.00675\n",
            "Iteration 14: MSE = 0.00667\n",
            "Iteration 15: MSE = 0.00658\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.2987074215035515, 0.012830140526624689)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converge at small threshold with Learning Rate"
      ],
      "metadata": {
        "id": "6K1GZk0tCeuu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent_stop_at_zero_mse(x, y, w0_init, w1_init, learning_rate):\n",
        "    w0 = w0_init\n",
        "    w1 = w1_init\n",
        "    n = len(x)  # Number of data points\n",
        "    iteration = 0\n",
        "    while True:\n",
        "        iteration += 1\n",
        "        # Calculate predictions\n",
        "        y_pred = [w0 + w1 * xi for xi in x]\n",
        "        # Calculate the gradients for w0 and w1\n",
        "        w0_gradient = sum((yp - yi) for yp, yi in zip(y_pred, y)) / n\n",
        "        w1_gradient = sum(((yp - yi) * xi) for yp, yi, xi in zip(y_pred, y, x)) / n\n",
        "        # Update the weights using the gradients\n",
        "        w0 -= learning_rate * w0_gradient\n",
        "        w1 -= learning_rate * w1_gradient\n",
        "        # Calculate the Mean Squared Error (MSE) for the current iteration\n",
        "        mse = sum((yi - yp)**2 for yi, yp in zip(y, y_pred)) / n\n",
        "        # Break the loop if MSE is close to 0\n",
        "        if mse < 1e-6:  # Using a small threshold instead of exactly 0 for practical reasons\n",
        "            break\n",
        "        # Optionally, to monitor the progress, you might print the iteration number and MSE\n",
        "        # print(f\"Iteration {iteration}: MSE = {mse:.5f}\")\n",
        "\n",
        "    return w0, w1, iteration\n",
        "\n",
        "# Given values\n",
        "x = [0.1, 0.2, 0.3]\n",
        "y = [0.2, 0.3, 0.4]\n",
        "w0_init = 1\n",
        "w1_init = 0.05\n",
        "learning_rate = 1\n",
        "\n",
        "# Call the modified function\n",
        "final_w0, final_w1, num_iterations = gradient_descent_stop_at_zero_mse(x, y, w0_init, w1_init, learning_rate)\n",
        "print(f\"Gradient descent converged after {num_iterations} iterations.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLqn3u3XChH4",
        "outputId": "1551e440-68fc-4609-fa4d-9a1150b1f13e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient descent converged after 699 iterations.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent_stop_at_zero_mse(x, y, w0_init, w1_init, learning_rate):\n",
        "    w0 = w0_init\n",
        "    w1 = w1_init\n",
        "    n = len(x)  # Number of data points\n",
        "    iteration = 0\n",
        "    while True:\n",
        "        iteration += 1\n",
        "        # Calculate predictions\n",
        "        y_pred = [w0 + w1 * xi for xi in x]\n",
        "        # Calculate the gradients for w0 and w1\n",
        "        w0_gradient = sum((yp - yi) for yp, yi in zip(y_pred, y)) / n\n",
        "        w1_gradient = sum(((yp - yi) * xi) for yp, yi, xi in zip(y_pred, y, x)) / n\n",
        "        # Update the weights using the gradients\n",
        "        w0 -= learning_rate * w0_gradient\n",
        "        w1 -= learning_rate * w1_gradient\n",
        "        # Calculate the Mean Squared Error (MSE) for the current iteration\n",
        "        mse = sum((yi - yp)**2 for yi, yp in zip(y, y_pred)) / n\n",
        "        # Break the loop if MSE is close to 0\n",
        "        if mse < 1e-6:  # Using a small threshold instead of exactly 0 for practical reasons\n",
        "            break\n",
        "        # Optionally, to monitor the progress, you might print the iteration number and MSE\n",
        "        # print(f\"Iteration {iteration}: MSE = {mse:.5f}\")\n",
        "\n",
        "    return w0, w1, iteration\n",
        "\n",
        "# Given values\n",
        "x = [0.1, 0.2, 0.3]\n",
        "y = [0.2, 0.3, 0.4]\n",
        "w0_init = 1\n",
        "w1_init = 0.05\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Call the modified function\n",
        "final_w0, final_w1, num_iterations = gradient_descent_stop_at_zero_mse(x, y, w0_init, w1_init, learning_rate)\n",
        "print(f\"Gradient descent converged after {num_iterations} iterations.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlIQx59MCpnW",
        "outputId": "9b4db149-90cc-4ac1-9750-8a36982def0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient descent converged after 70000 iterations.\n"
          ]
        }
      ]
    }
  ]
}